{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMLHTJ2cP7zpKV6F2UVH/h4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jford59/RT1/blob/main/HW3/RT3_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBR01G5TmCda",
        "outputId": "aa7f17a3-d05e-4ecd-d1bb-fd1ae00d256f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM Sequence 20\n",
            "Epoch [1/10], Training Loss: 1.8255, Validation Loss: 1.6367, Validation Accuracy: 51.07%\n",
            "Epoch [2/10], Training Loss: 1.5737, Validation Loss: 1.5502, Validation Accuracy: 53.18%\n",
            "Epoch [3/10], Training Loss: 1.5088, Validation Loss: 1.5067, Validation Accuracy: 54.43%\n",
            "Epoch [4/10], Training Loss: 1.4728, Validation Loss: 1.4795, Validation Accuracy: 55.11%\n",
            "Epoch [5/10], Training Loss: 1.4484, Validation Loss: 1.4658, Validation Accuracy: 55.43%\n",
            "Epoch [6/10], Training Loss: 1.4303, Validation Loss: 1.4557, Validation Accuracy: 55.57%\n",
            "Epoch [7/10], Training Loss: 1.4167, Validation Loss: 1.4408, Validation Accuracy: 56.08%\n",
            "Epoch [8/10], Training Loss: 1.4049, Validation Loss: 1.4336, Validation Accuracy: 56.22%\n",
            "Epoch [9/10], Training Loss: 1.3956, Validation Loss: 1.4314, Validation Accuracy: 56.39%\n",
            "Epoch [10/10], Training Loss: 1.3877, Validation Loss: 1.4247, Validation Accuracy: 56.20%\n",
            "GRU Sequence 20\n",
            "Epoch [1/10], Training Loss: 1.7960, Validation Loss: 1.6224, Validation Accuracy: 51.53%\n",
            "Epoch [2/10], Training Loss: 1.5671, Validation Loss: 1.5486, Validation Accuracy: 53.47%\n",
            "Epoch [3/10], Training Loss: 1.5140, Validation Loss: 1.5185, Validation Accuracy: 54.01%\n",
            "Epoch [4/10], Training Loss: 1.4857, Validation Loss: 1.4955, Validation Accuracy: 54.78%\n",
            "Epoch [5/10], Training Loss: 1.4679, Validation Loss: 1.4878, Validation Accuracy: 54.73%\n",
            "Epoch [6/10], Training Loss: 1.4549, Validation Loss: 1.4739, Validation Accuracy: 55.18%\n",
            "Epoch [7/10], Training Loss: 1.4443, Validation Loss: 1.4707, Validation Accuracy: 55.27%\n",
            "Epoch [8/10], Training Loss: 1.4368, Validation Loss: 1.4664, Validation Accuracy: 55.34%\n",
            "Epoch [9/10], Training Loss: 1.4302, Validation Loss: 1.4615, Validation Accuracy: 55.40%\n",
            "Epoch [10/10], Training Loss: 1.4259, Validation Loss: 1.4606, Validation Accuracy: 55.54%\n",
            "LSTM: Predicted next character: 'g'\n",
            "LSTM Sequence 30\n",
            "Epoch [1/10], Training Loss: 1.8343, Validation Loss: 1.6269, Validation Accuracy: 51.48%\n",
            "Epoch [2/10], Training Loss: 1.5773, Validation Loss: 1.5365, Validation Accuracy: 53.76%\n",
            "Epoch [3/10], Training Loss: 1.5110, Validation Loss: 1.4955, Validation Accuracy: 54.84%\n",
            "Epoch [4/10], Training Loss: 1.4733, Validation Loss: 1.4719, Validation Accuracy: 55.45%\n",
            "Epoch [5/10], Training Loss: 1.4484, Validation Loss: 1.4550, Validation Accuracy: 55.93%\n",
            "Epoch [6/10], Training Loss: 1.4299, Validation Loss: 1.4421, Validation Accuracy: 56.29%\n",
            "Epoch [7/10], Training Loss: 1.4155, Validation Loss: 1.4333, Validation Accuracy: 56.45%\n",
            "Epoch [8/10], Training Loss: 1.4033, Validation Loss: 1.4263, Validation Accuracy: 56.70%\n",
            "Epoch [9/10], Training Loss: 1.3937, Validation Loss: 1.4176, Validation Accuracy: 56.88%\n",
            "Epoch [10/10], Training Loss: 1.3848, Validation Loss: 1.4141, Validation Accuracy: 56.82%\n",
            "GRU Sequence 30\n",
            "Epoch [1/10], Training Loss: 1.7941, Validation Loss: 1.6067, Validation Accuracy: 51.96%\n",
            "Epoch [2/10], Training Loss: 1.5664, Validation Loss: 1.5319, Validation Accuracy: 53.93%\n",
            "Epoch [3/10], Training Loss: 1.5128, Validation Loss: 1.5001, Validation Accuracy: 54.86%\n",
            "Epoch [4/10], Training Loss: 1.4834, Validation Loss: 1.4852, Validation Accuracy: 55.30%\n",
            "Epoch [5/10], Training Loss: 1.4654, Validation Loss: 1.4725, Validation Accuracy: 55.46%\n",
            "Epoch [6/10], Training Loss: 1.4523, Validation Loss: 1.4653, Validation Accuracy: 55.72%\n",
            "Epoch [7/10], Training Loss: 1.4421, Validation Loss: 1.4592, Validation Accuracy: 55.86%\n",
            "Epoch [8/10], Training Loss: 1.4342, Validation Loss: 1.4577, Validation Accuracy: 55.86%\n",
            "Epoch [9/10], Training Loss: 1.4284, Validation Loss: 1.4497, Validation Accuracy: 56.20%\n",
            "Epoch [10/10], Training Loss: 1.4226, Validation Loss: 1.4459, Validation Accuracy: 56.10%\n",
            "LSTM: Predicted next character: 'g'\n",
            "LSTM Sequence 50\n",
            "Epoch [1/10], Training Loss: 1.8281, Validation Loss: 1.6315, Validation Accuracy: 51.00%\n",
            "Epoch [2/10], Training Loss: 1.5712, Validation Loss: 1.5350, Validation Accuracy: 53.84%\n",
            "Epoch [3/10], Training Loss: 1.5024, Validation Loss: 1.4996, Validation Accuracy: 54.64%\n",
            "Epoch [4/10], Training Loss: 1.4635, Validation Loss: 1.4711, Validation Accuracy: 55.50%\n",
            "Epoch [5/10], Training Loss: 1.4380, Validation Loss: 1.4498, Validation Accuracy: 56.04%\n",
            "Epoch [6/10], Training Loss: 1.4192, Validation Loss: 1.4347, Validation Accuracy: 56.29%\n",
            "Epoch [7/10], Training Loss: 1.4042, Validation Loss: 1.4303, Validation Accuracy: 56.53%\n",
            "Epoch [8/10], Training Loss: 1.3927, Validation Loss: 1.4226, Validation Accuracy: 56.73%\n",
            "Epoch [9/10], Training Loss: 1.3830, Validation Loss: 1.4186, Validation Accuracy: 56.88%\n",
            "Epoch [10/10], Training Loss: 1.3740, Validation Loss: 1.4109, Validation Accuracy: 56.88%\n",
            "GRU Sequence 50\n",
            "Epoch [1/10], Training Loss: 1.8005, Validation Loss: 1.6270, Validation Accuracy: 51.29%\n",
            "Epoch [2/10], Training Loss: 1.5676, Validation Loss: 1.5306, Validation Accuracy: 53.94%\n",
            "Epoch [3/10], Training Loss: 1.5016, Validation Loss: 1.4993, Validation Accuracy: 54.81%\n",
            "Epoch [4/10], Training Loss: 1.4713, Validation Loss: 1.4788, Validation Accuracy: 55.08%\n",
            "Epoch [5/10], Training Loss: 1.4516, Validation Loss: 1.4714, Validation Accuracy: 55.35%\n",
            "Epoch [6/10], Training Loss: 1.4380, Validation Loss: 1.4546, Validation Accuracy: 55.82%\n",
            "Epoch [7/10], Training Loss: 1.4273, Validation Loss: 1.4491, Validation Accuracy: 56.11%\n",
            "Epoch [8/10], Training Loss: 1.4183, Validation Loss: 1.4443, Validation Accuracy: 56.15%\n",
            "Epoch [9/10], Training Loss: 1.4114, Validation Loss: 1.4413, Validation Accuracy: 56.21%\n",
            "Epoch [10/10], Training Loss: 1.4064, Validation Loss: 1.4385, Validation Accuracy: 56.47%\n"
          ]
        }
      ],
      "source": [
        "# Importing necessary modules\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import requests\n",
        "\n",
        "# Fetching text data from a URL\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "\n",
        "# Creating character mappings\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
        "int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "# Encoding the text data\n",
        "encoded_text = [char_to_int[ch] for ch in text]\n",
        "\n",
        "# Checking for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Function to define dataset for given max_length\n",
        "def Define_Dataset(max_length):\n",
        "    train = []\n",
        "    test = []\n",
        "    for i in range(len(text) - max_length):\n",
        "        sequence = text[i:i + max_length]\n",
        "        label = text[i + max_length]\n",
        "        train.append([char_to_int[char] for char in sequence])\n",
        "        test.append(char_to_int[label])\n",
        "\n",
        "    train = np.array(train)\n",
        "    test = np.array(test)\n",
        "    return train, test\n",
        "\n",
        "# Creating datasets for different sequence lengths\n",
        "train20, test20 = Define_Dataset(20)\n",
        "train30, test30 = Define_Dataset(30)\n",
        "train50, test50 = Define_Dataset(50)\n",
        "\n",
        "# Converting datasets to PyTorch tensors\n",
        "train20 = torch.tensor(train20, dtype=torch.long)\n",
        "test20 = torch.tensor(test20, dtype=torch.long)\n",
        "\n",
        "train30 = torch.tensor(train30, dtype=torch.long)\n",
        "test30 = torch.tensor(test30, dtype=torch.long)\n",
        "\n",
        "train50 = torch.tensor(train50, dtype=torch.long)\n",
        "test50 = torch.tensor(test50, dtype=torch.long)\n",
        "\n",
        "# Defining the RNN model\n",
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(CharRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        #This line takes the input tensor x, which contains indices of characters, and passes it through an embedding layer (self.embedding).\n",
        "        #The embedding layer converts these indices into dense vectors of fixed size.\n",
        "        #These vectors are learned during training and can capture semantic similarities between characters.\n",
        "        #The result is a higher-dimensional representation of the input sequence, where each character index is replaced by its corresponding embedding vector.\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        #The RNN layer returns two outputs:\n",
        "        #1- the output tensor containing the output of the RNN at each time step for each sequence in the batch,\n",
        "        #2-the hidden state (_) of the last time step (which is not used in this line, hence the underscore).\n",
        "        output, _ = self.rnn(embedded)\n",
        "        #The RNN's output contains the outputs for every time step,\n",
        "        #but for this task, we're only interested in the output of the last time step because we're predicting the next character after the sequence.\n",
        "        #output[:, -1, :] selects the last time step's output for every sequence in the batch (-1 indexes the last item in Python).\n",
        "        output = self.fc(output[:, -1, :])  # Get the output of the last RNN cell\n",
        "        return output\n",
        "\n",
        "# Hyperparameters\n",
        "hidden_size = 128\n",
        "learning_rate = 0.005\n",
        "epochs = 100\n",
        "\n",
        "# Defining custom dataset class\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, sequences, targets):\n",
        "        self.sequences = sequences\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.sequences[index], self.targets[index]\n",
        "\n",
        "# Creating datasets and dataloaders for different sequence lengths\n",
        "dataset_20 = CharDataset(train20, test20)\n",
        "dataset_30 = CharDataset(train30, test30)\n",
        "dataset_50 = CharDataset(train50, test50)\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "train_size20 = int(len(dataset_20) * 0.8)\n",
        "test_size20 = len(dataset_20) - train_size20\n",
        "train20_dataset, test20_dataset = torch.utils.data.random_split(dataset_20, [train_size20, test_size20])\n",
        "\n",
        "train20_loader = DataLoader(train20_dataset, shuffle=True, batch_size=batch_size)\n",
        "test20_loader = DataLoader(test20_dataset, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "train_size30 = int(len(dataset_30) * 0.8)\n",
        "test_size30 = len(dataset_30) - train_size30\n",
        "train30_dataset, test30_dataset = torch.utils.data.random_split(dataset_30, [train_size30, test_size30])\n",
        "\n",
        "train30_loader = DataLoader(train30_dataset, shuffle=True, batch_size=batch_size)\n",
        "test30_loader = DataLoader(test30_dataset, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "train_size50 = int(len(dataset_50) * 0.8)\n",
        "test_size50 = len(dataset_50) - train_size50\n",
        "train50_dataset, test50_dataset = torch.utils.data.random_split(dataset_50, [train_size50, test_size50])\n",
        "\n",
        "train50_loader = DataLoader(train50_dataset, shuffle=True, batch_size=batch_size)\n",
        "test50_loader = DataLoader(test50_dataset, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "# Defining LSTM model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, _ = self.lstm(embedded)\n",
        "        output = self.fc(output[:, -1, :])\n",
        "        return output\n",
        "\n",
        "# Defining GRU model\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, _ = self.gru(embedded)\n",
        "        output = self.fc(output[:, -1, :])\n",
        "        return output\n",
        "\n",
        "# Function for the training loop\n",
        "def training_loop(train, test, model, loss_fn, optimizer, epochs):\n",
        "    model.to(device)\n",
        "    train_loss_list = []\n",
        "    val_loss_list = []\n",
        "    val_accuracy_list = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = 0.0\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        model.train()\n",
        "        for sequences, targets in train:\n",
        "            sequences, targets = sequences.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(sequences)\n",
        "            loss = loss_fn(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * sequences.size(0)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for sequences, targets in test:\n",
        "                sequences, targets = sequences.to(device), targets.to(device)\n",
        "                outputs = model(sequences)\n",
        "                loss = loss_fn(outputs, targets)\n",
        "                val_loss += loss.item() * sequences.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += targets.size(0)\n",
        "                correct += (predicted == targets).sum().item()\n",
        "\n",
        "        train_loss = train_loss / len(train.dataset)\n",
        "        val_loss = val_loss / len(test.dataset)\n",
        "        accuracy = correct / total * 100\n",
        "\n",
        "        train_loss_list.append(train_loss)\n",
        "        val_loss_list.append(val_loss)\n",
        "        val_accuracy_list.append(accuracy)\n",
        "\n",
        "        print(f'Epoch [{epoch + 1}/{epochs}], '\n",
        "              f'Training Loss: {train_loss:.4f}, '\n",
        "              f'Validation Loss: {val_loss:.4f}, '\n",
        "              f'Validation Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# Function to predict the next character\n",
        "def predict_next_char(model, sequence_length, char_to_int, int_to_char, test_str):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_sequence = [char_to_int[char] for char in test_str]\n",
        "        test_sequence = torch.tensor(test_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "        output = model(test_sequence)\n",
        "        _, predicted_index = torch.max(output, 1)\n",
        "        predicted_char = int_to_char[predicted_index.item()]\n",
        "    return predicted_char\n",
        "\n",
        "# Hyperparameters\n",
        "hidden_size = 128\n",
        "learning_rate = 0.001\n",
        "epochs = 10\n",
        "\n",
        "# Creating LSTM and GRU models for sequence length 20\n",
        "LSTM20_Model = LSTMModel(len(chars), hidden_size, len(chars))\n",
        "GRU20_Model = GRUModel(len(chars), hidden_size, len(chars))\n",
        "\n",
        "# Defining loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Defining optimizers for LSTM and GRU models\n",
        "LSTM20_optimizer = optim.Adam(LSTM20_Model.parameters(), lr=learning_rate)\n",
        "GRU20_optimizer = optim.Adam(GRU20_Model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training LSTM and GRU models for sequence length 20\n",
        "print(\"LSTM Sequence 20\")\n",
        "training_loop(\n",
        "    train=train20_loader,\n",
        "    test=test20_loader,\n",
        "    model=LSTM20_Model,\n",
        "    loss_fn=criterion,\n",
        "    optimizer=LSTM20_optimizer,\n",
        "    epochs=epochs\n",
        ")\n",
        "print(\"GRU Sequence 20\")\n",
        "training_loop(\n",
        "    train=train20_loader,\n",
        "    test=test20_loader,\n",
        "    model=GRU20_Model,\n",
        "    loss_fn=criterion,\n",
        "    optimizer=GRU20_optimizer,\n",
        "    epochs=epochs\n",
        ")\n",
        "\n",
        "# Predicting the next character using LSTM model for sequence length 20\n",
        "test_str = \"This is a simple example to demonstrate how to predict the next char\"\n",
        "predicted_char = predict_next_char(LSTM20_Model, 20, char_to_int, int_to_char, test_str)\n",
        "print(f\"LSTM: Predicted next character: '{predicted_char}'\")\n",
        "\n",
        "# Creating LSTM and GRU models for sequence length 30\n",
        "LSTM30_Model = LSTMModel(len(chars), hidden_size, len(chars))\n",
        "GRU30_Model = GRUModel(len(chars), hidden_size, len(chars))\n",
        "\n",
        "# Defining loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Defining optimizers for LSTM and GRU models\n",
        "LSTM30_optimizer = optim.Adam(LSTM30_Model.parameters(), lr=learning_rate)\n",
        "GRU30_optimizer = optim.Adam(GRU30_Model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training LSTM and GRU models for sequence length 30\n",
        "print(\"LSTM Sequence 30\")\n",
        "training_loop(\n",
        "    train=train30_loader,\n",
        "    test=test30_loader,\n",
        "    model=LSTM30_Model,\n",
        "    loss_fn=criterion,\n",
        "    optimizer=LSTM30_optimizer,\n",
        "    epochs=epochs\n",
        ")\n",
        "print(\"GRU Sequence 30\")\n",
        "training_loop(\n",
        "    train=train30_loader,\n",
        "    test=test30_loader,\n",
        "    model=GRU30_Model,\n",
        "    loss_fn=criterion,\n",
        "    optimizer=GRU30_optimizer,\n",
        "    epochs=epochs\n",
        ")\n",
        "\n",
        "# Predicting the next character using LSTM model for sequence length 30\n",
        "test_str = \"This is a simple example to demonstrate how to predict the next char\"\n",
        "predicted_char = predict_next_char(LSTM30_Model, 30, char_to_int, int_to_char, test_str)\n",
        "print(f\"LSTM: Predicted next character: '{predicted_char}'\")\n",
        "\n",
        "# Creating LSTM and GRU models for sequence length 50\n",
        "LSTM50_Model = LSTMModel(len(chars), hidden_size, len(chars))\n",
        "GRU50_Model = GRUModel(len(chars), hidden_size, len(chars))\n",
        "\n",
        "# Defining loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Defining optimizers for LSTM and GRU models\n",
        "LSTM50_optimizer = optim.Adam(LSTM50_Model.parameters(), lr=learning_rate)\n",
        "GRU50_optimizer = optim.Adam(GRU50_Model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training LSTM and GRU models for sequence length 50\n",
        "print(\"LSTM Sequence 50\")\n",
        "training_loop(\n",
        "    train=train50_loader,\n",
        "    test=test50_loader,\n",
        "    model=LSTM50_Model,\n",
        "    loss_fn=criterion,\n",
        "    optimizer=LSTM50_optimizer,\n",
        "    epochs=epochs\n",
        ")\n",
        "print(\"GRU Sequence 50\")\n",
        "training_loop(\n",
        "    train=train50_loader,\n",
        "    test=test50_loader,\n",
        "    model=GRU50_Model,\n",
        "    loss_fn=criterion,\n",
        "    optimizer=GRU50_optimizer,\n",
        "    epochs=epochs\n",
        ")"
      ]
    }
  ]
}